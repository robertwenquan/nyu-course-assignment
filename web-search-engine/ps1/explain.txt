an explain.txt file containing a precise and 
succint description of how the program works and 
what the major functions are. 

Also, a list of any bugs or non-working features in your program. 

Finally, a list of any special features beyond the 
basic requirements, if there are any.

1. How to run:
  $ python crawln.py [-n [NUM]] [keyword [keyword ...]]

  Default with search 'nyu poly' with 10 pages
  $ python crawln.py

  The following launch will search 'nyu poly computer science' and crawl 1000 pages
  $ python crawln.py -n 1000 'nyu poly computer science'

2. How does it work:
  Start:
    Send google search query, get returned urls and put into crawl queue
    If somehow no results come back from Google, like the recapcha is prompted, try Bing search
    If still no results come back from Bing, stop the crawl and prompt for network failure

  Run:
    Read queue as a loop, exit when the queue has been empty for 30s or crawled item reached N

3. Major functions:

  Parse pages:
    fetch the page and parse it
      1. Before fetching the page, the header of the file was fetched first
         content-type is checked. non 'text/html' MIME type will be simply ignored
      2. blacklist of file extensions are checked against the URL
      3. the page is fetched and analyzed with TF (term frequency)
      4. children links in this page are added to the crawl queue
    
  Score strategy:
    Based on TF (term frequency)
    1. Inherit 1/3 priority score of its parent page
    2. Give higher weight of terms appeared in url and title
    3. Encourage parsing pages with diverse keywords, measured by Entropy
    4. Use max operation when one page is linked by multiple pages

  Validation check:
    1. Decode URL and turn it into simplest form
    2. Avoid URLs with unexpected extension
    3. Filter URLs with non http or https protocol
    4. Give up pages that is disallowed by robot crawler

  Crawl logs:
    1. all crawled pages are logged, upon success or failure
    2. HTTP status code, crawl page size, crawl start time, crawl duration, queued time, page depth, etc. are logged
    3. logged in JSONL format

  Crawl statistics:
    The following statistics are logged for the crawl:
    - crawl start time
    - crawl end time
    - crawl duration in seconds
    - crawled number of pages
    - crawled page in bytes
    - crawled page in successful state
    - crawled page in success rate

== Known Issue

 * Not fully supporting robots.txt
 * Not handling supporting password protected page
 * Crawling is not fast enough
 * Sync issue in log writing

== Extra Features

 * Multi-threading
  * we use threading library to parallel the crawling 

 * robots.txt checking
  * only DisAllow is supported for the robots.txt protocol

 * Fake crawl mode
  * add --fake mode for testing and debugging
  * This is quite useful without network connection

== TODO in the future
 * scale to AWS

