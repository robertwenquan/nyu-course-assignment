<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
  <link rel="stylesheet" title="default stylesheet" media="screen" type="text/css" href="http://cdn4.explainthatstuff.com/ets2.css">
  <link rel="canonical" href="http://www.explainthatstuff.com/voicerecognition.html">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">
  <title>How does voice recognition software work? - Explain that Stuff</title>
  <meta name="description" content="A simple introduction to computer voice recognition programs that understand your spoken words.">
<link rel="shortcut icon" href="http://cdn4.explainthatstuff.com/favicon.ico">

<!-- Begin Cookie Consent plugin. Original version by Silktide (http://silktide.com/cookieconsent), slightly modified by
explainthatstuff.com on July 29, 2015), released under GNU General Public License Version (http://www.gnu.org/licenses/gpl.txt) as published by the Free Software Foundation.
-->
<script type="text/javascript">
    window.cookieconsent_options = {"message":"We use cookies to improve your experience of our site and to serve relevant, personalized advertising.","dismiss":"Got it!","learnMore":"More info","link":"http://www.explainthatstuff.com/privacy.html","theme":"light-bottom"};
</script>

<script type="text/javascript" src="http://cdn4.explainthatstuff.com/cookieconsent3.latest.min.js"></script>
<!-- End Cookie Consent plugin -->
</head>

<body itemscope itemtype="http://schema.org/Article"><div class="container"><a name="pagetop"></a>



<div class="header"><IMG class="headlogo" ALT="Explain that Stuff" src="http://cdn4.explainthatstuff.com/dot.gif"></div>

<div class="searchtop">
<!-- SiteSearch Google top search start -->
<form action="http://www.explainthatstuff.com/search.html" id="cse-search-box">
  <div>
    <input type="hidden" name="cx" value="partner-pub-1030585152417294:7662706493" />
    <input type="hidden" name="cof" value="FORID:10" />
    <input type="hidden" name="ie" value="UTF-8" />
    <input type="text" name="q" size="40" />
    <input type="submit" name="sa" value="Search" />
  </div>
</form>
<!-- SiteSearch Google top search end -->
</div>

<!-- Top ad removed -->
<!-- Ad placement: computing -->



<div class="crumb">
You are here:
<a href="/">Home page</a> &gt;
<a href="articles_computers.html">Computers</A> &gt;
Voice recognition software
</div>


<!-- old nav bar -->



<div class="nav">
<ul class="nobull"><li class="inline"><a href="/">Home</a></li>
<li class="inline"><a href="azindex.html">A-Z index</a></li>
<li class="inline"><a href="mybooks.php">Get the book</a></li>
<li class="inline"><a href="followus.html">Follow us</a></li>
<li class="inline"><a href="random.php">Random article</a></li>
<li class="inline"><a href="timeline.html">Timeline</a></li>
<li class="inline"><a href="teaching-guide.html">Teaching guide</a></li>
<li class="inline"><a href="aboutus.html">About us</a></li>
<li class="inline2"><a href="privacy.html">Privacy policy</a></li>
</ul>
</div>

<div class="adrightsm">Advertisement
<script async src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:inline-block;width:300px;height:250px"
     data-ad-client="ca-pub-1030585152417294"
     data-ad-slot="1317849019"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
</div>



<div class="main2">
<p><img itemprop="image" src="http://cdn4.explainthatstuff.com/voice-recognition-microphone.jpg" class="ileftno" height="190" width="320"  ALT="A voice dictation microphone"></p>
<h1 itemprop="headline name">Voice recognition software</h1>


<div class="sharetop"><ul class="nobull">
<li class="shr"><iframe src="http://www.facebook.com/plugins/like.php?app_id=214994648542122&amp;href=http%3A%2F%2Fwww.explainthatstuff.com%2Fvoicerecognition.html&amp;send=false&amp;layout=button_count&amp;width=90&amp;show_faces=false&amp;action=like&amp;colorscheme=light&amp;font&amp;height=21" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:90px; height:21px;" allowTransparency="true"></iframe></li>
<!-- plusone and tweet -->
<!-- sharebuttons -->
<li class="shr"><g:plusone size="medium"></g:plusone></li>
<li class="shr"><a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal">Tweet</a></li> 
</ul></div>
<p>by <a href="chris-woodford.html">Chris Woodford</a>. <span class="italic">Last updated: June 7, 2015. </span></p>
<meta itemprop="alternativeHeadline" content="How does voice recognition work?">
<meta itemprop="description" content="How can a computer turn the sound of your voice into editable words on your screen?">
<meta itemprop="datePublished" content="2007-08-30">
<meta itemprop="dateModified" content="2015-06-07">

<p itemprop="articleBody"><span class="dropcap">I</span>t's just as well people can understand speech. Imagine if you
were like a computer: friends would have to "talk" to you by
prodding away at a plastic keyboard connected to your brain by a
long, curly wire. If you wanted to say "hello" to someone, you'd
have to reach out, chatter your fingers over <i>their</i> keyboard,
and wait for their eyes to light up; they'd have to do the same to
you. Conversations would be a long, slow, elaborate nightmare&mdash;a
silent dance of fingers on plastic; strange, abstract, and remote.
We'd never put up with such clumsiness as humans, so why do we talk
to our computers this way?</p>

<p>Scientists have long dreamed of building machines that
can chatter and listen just like humans. But although computerized
<B>speech recognition</B> has been around for decades, and is now built
into most smartphones and PCs, few of us actually use it. Why?
Possibly because we never even bother to try it out, working on the
assumption that computers could never pull off a trick so complex as
understanding the human voice. It's certainly true that speech
recognition is a complex problem that's challenged some of the
world's best computer scientists, mathematicians, and linguists. How
well are they doing at cracking the problem? Will we all be chatting
to our PCs one day soon? Let's take a closer look and find out!</p>

<p><span class="credit">Photo: Using a headset <a href="microphones.html">microphone</a> like this makes a huge difference to the accuracy of speech recognition: it reduces background sound, making it much easier
for the computer to separate the signal (the all-important words you're speaking) from the noise (everything else).</span></p>

<h2>Getting your head around speech</h2>

<p><img src="http://cdn4.explainthatstuff.com/ipod-voice-control.jpg" height="300" width="225" class="irightno" ALT="Controlling an iPod with its built-in voice recognition."></P>

<p>Language sets people far above our creeping, crawling animal
friends. While the more intelligent creatures, such as dogs and
dolphins, certainly know how to communicate with sounds, only humans
enjoy the rich complexity of language. With just a couple of dozen
letters, we can build any number of words (most dictionaries contain
tens of thousands) and express an infinite number of thoughts.</p>

<p>When we speak, our voices generate little sound packets called
<b>phones</b> (which correspond to the sounds of letters or groups of
letters in words); so speaking the word cat produces phones that correspond
to the sounds "c," "a," and "t." Although you've probably never heard
of these kinds of phones before, you might well be familiar with the
related concept of <b>phonemes</b>: simply speaking, phonemes are the
basic LEGO&trade; blocks of sound that all words are built from.
Although the difference between phones and phonemes is complex and
can be very confusing, this is one "quick-and-dirty" way to
remember it: phones are <i>actual</i> bits of sound that we speak
(real, concrete things), whereas phonemes are <i>ideal</i> bits of
sound we store (in some sense) in our minds (abstract, theoretical
sound fragments that are never actually spoken).</p>

<p><a href="howcomputerswork.html">Computers</A> and
<a href="how-computer-models-work.html">computer models</a> can juggle around with phonemes, but
the real bits of speech they analyze always involves processing
phones. When we listen to speech, our ears catch phones flying
through the air and our leaping brains flip them back into words,
sentences, thoughts, and ideas&mdash;so quickly, that we often know what
people are going to say before the words have fully fled from their
mouths. Instant, easy, and quite dazzling, our amazing brains make
this seem like a magic trick. And it's perhaps because listening
seems so easy to us that we think computers (in many ways even more
amazing than brains) should be able to hear, recognize, and decode
spoken words as well. If only it were that simple!</p>

<p><span class="credit">Photo: Speech recognition is popping up all over the place. iPods have built-in "voice control" programs that let you pick out music just by saying "Play albums by U2," or whatever band you're in the mood for.</span></p>


<h3>Why is speech so hard to handle?</h3>

<p>The trouble is, listening is much harder than it looks (or
sounds): there are all sorts of different problems going on at the same
time...</P>
<UL>
<LI>When someone speaks to you in the street, there's the sheer
difficulty of separating their words (what scientists would call the
acoustic <b>signal</b>) from the background <b>noise</b>&mdash;especially
in something like a cocktail party, where the "noise" is
similar speech from other conversations.</LI>
<LI>When people talk quickly, and run all their words together in a long stream, how do we know
exactly when one word ends and the next one begins? (Did they just
say "dancing and smile" or "dance, sing, and smile"?)</LI>
<LI>There's the problem of how everyone's voice is a little bit
different, and the way our voices change from moment to moment. How
do our brains figure out that a word like "bird" means exactly
the same thing when it's trilled by a ten year-old girl or boomed by
her forty-year-old father?</LI>
<LI>What about words like "red" and "read" that sound identical but mean totally different things (homophones,
as they're called)? How does our brain know which word the speaker
means?</LI>
<LI>What about sentences that are misheard to mean radically
different things? There's the age-old military example of "send
reinforcements, we're going to advance" being misheard for "send
three and fourpence, we're going to a dance"&mdash;and all of us can
probably think of song lyrics we've hilariously misunderstood the
same way (I always chuckle when I hear Kate Bush singing about "the
cattle burning over your shoulder").</LI>
</UL>
<P>On top of all that stuff, there are issues like
<B>syntax</B> (the grammatical structure of language) and <B>semantics</B> (the
meaning of words) and how they help our brain decode the words we
hear, as we hear them. Weighing up all these factors, it's easy to
see that recognizing and understanding spoken words in real time (as
people speak to us) is an astonishing demonstration of blistering
brainpower.</p>

<p>It shouldn't surprise or disappoint us that computers struggle to
pull off the same dazzling tricks as our brains; it's
quite amazing that they get anywhere near!</p>

<h2>How do computers recognize speech?</h2>

<p>Speech recognition is one of the most complex areas of computer
science&mdash;and partly because it's interdisciplinary: it involves a mixture of
extremely complex linguistics, mathematics, and computing itself. If
you read through some of the technical and scientific papers that have been published
in this area (a few are listed in the references below), you may well
struggle to make sense of the complexity. My
objective is to give a rough flavor of how computers recognize
speech, so&mdash;without any apology whatsoever&mdash;I'm going to simplify
hugely and miss out most of the details.</p>

<p>Broadly speaking, there are four different approaches a computer
can take if it wants to turn spoken sounds into written words:</p> 
<OL>
<LI>Simple pattern matching (where each spoken word is recognized in its
entirety&mdash;the way you instantly recognize a tree or a table without
consciously analyzing what you're looking at)</LI>
<LI>Pattern and feature
analysis (where each word is broken into bits and recognized from key
features, such as the vowels it contains)</LI>
<LI>Language modeling and statistical analysis (in which a knowledge of grammar and the
probability of certain words or sounds following on from one another is
used to speed up recognition and improve accuracy)</LI>
<LI>Artificial neural networks (brain-like computer models that can reliably
recognize patterns, such as word sounds, after exhaustive training).</LI>
</OL>

<P>In practice, the everyday speech recognition we encounter in things
like automated call centers, computer dictation software, or
smartphone "agents" (like Siri and Cortana) combines a variety
of different approaches. For the purposes of understanding clearly how things work,
however, it's best to keep things quite separate and think
about them one at a time.</p>



<h3>1: Simple pattern matching</h3>

<p><img src="http://cdn4.explainthatstuff.com/voice-activated-dialling.jpg" height="300" width="225" class="ileftno" ALT="Voice dialing on a Motorola cellphone."></P>

<p>Ironically, the simplest kind of speech recognition isn't really
anything of the sort. You'll have encountered it if you've ever
phoned an automated call center and been answered by a computerized
switchboard. Utility companies often have systems like this that you
can use to leave meter readings, and banks sometimes use them to
automate basic services like balance inquiries, statement orders,
checkbook requests, and so on. You simply dial a number, wait for a
recorded voice to answer, then either key in or speak your account
number before pressing more keys (or speaking again) to select what
you want to do. Crucially, all you ever get to do is choose one
option from a very short list, so the computer at the other end never
has to do anything as complex as <b>parsing</b> a sentence (splitting
a string of spoken sound into separate words and figuring out their
structure), much less trying to understand it; it needs no knowledge
of syntax (language structure) or semantics (meaning). In other words, systems like this aren't really <i>recognizing</i> speech at all: they simply have to be able to <i>distinguish</i>
between ten different sound patterns (the spoken words zero through
nine) either using the bleeping sounds of a Touch-Tone phone keypad
(technically called <a href="http://www.genave.com/dtmf.htm" target="_blank">DTMF</A>) or the spoken sounds of your voice.</p>

<p>From a computational point of view, there's not a huge difference
between recognizing phone tones and spoken numbers "zero", "one,"
"two," and so on: in each case, the system could solve the problem by
comparing an entire chunk of sound to similar stored patterns in its memory.
It's true that there can be quite a bit of variability in how different people say
"three" or "four" (they'll speak in a different tone, more or
less slowly, with different amounts of background noise) but the ten
numbers are sufficiently different from one another for this not to
present a huge computational challenge. And if the system can't
figure out what you're saying, it's easy enough for the call to be
transferred automatically to a human operator.</p>

<p><span class="credit">Photo: Voice-activated dialing on cellphones is little
more than simple pattern matching. You simply train the phone to recognize the spoken
version of a name in your phonebook.
When you say a name, the phone doesn't do any particularly sophisticated analysis;
it simply compares the sound pattern with ones you've stored previously and picks the best match.</span></p>

<h3>2: Pattern and feature analysis</h3>

<p>Automated switchboard systems generally work very reliably because
they have such tiny vocabularies: usually, just ten words
representing the ten basic digits. The vocabulary that a speech
system works with is sometimes called its <b>domain</b>. Early speech
systems were often optimized to work within very specific domains,
such as transcribing doctor's notes, computer programming commands,
or legal jargon, which made the speech recognition problem far
simpler (because the vocabulary was smaller and technical terms
were explicitly trained beforehand). Much like humans, modern
speech recognition programs are so good that they work in any domain
and can recognize tens of thousands of different words. How do they
do it?</p>

<p>Most of us have relatively large vocabularies, made from hundreds
of common words ("a," "the," "but" and so on, which we
hear many times each day) and thousands of less common ones (like
"discombobulate," "crepuscular," "balderdash," or
whatever, which we might not hear from one year to the next).
Theoretically, you could train a speech recognition system to
understand any number of different words, just like an automated
switchboard: all you'd need to do would be to get your speaker to
read each word three or four times into a microphone, until the
computer generalized the sound pattern into something it could
recognize reliably.</p>

<p>The trouble with this approach is that it's hugely inefficient.
Why learn to recognize every word in the dictionary when all those
words are built from the same basic set of sounds? No-one wants to
buy an off-the-shelf computer dictation system only to find they have
to read three or four times through a dictionary, training it up to
recognize every possible word they might ever speak, before they can
do anything useful. So what's the alternative? How do humans
do it? We don't need to have seen every Ford, Chevrolet, and Cadillac
ever manufactured to recognize that an unknown, four-wheeled vehicle
is a car: having seen many <b>examples</b> of cars throughout our
lives, our brains somehow store what's called a <b>prototype</b> (the
generalized concept of a car, something with four wheels, big enough
to carry two to four passengers, that creeps down a road) and we
figure out that an object we've never seen before is a car by
comparing it with the prototype. In much the same way, we don't need
to have heard every person on Earth read every word in the dictionary
before we can understand what they're saying; somehow we can
recognize words by analyzing the key features (or components) of the
sounds we hear. Speech recognition systems take the same approach.</p>

<h4>The recognition process</h4>

<p>Practical speech recognition systems start by listening to a chunk
of sound (technically called an <b>utterance</b>) read through a
microphone. The first step involves digitizing the sound (so the
up-and-down, <b>analog</b> wiggle of the sound waves is turned into
<b>digital</b> format, a string of numbers) by a piece of hardware
(or software) called an <b>analog-to-digital (A/D) converter</b>
(for a basic introduction, see our article on <a href="analog-and-digital.html">analog
versus digital technology</A>). The
digital data is converted into a <b>spectrogram</b> (a graph
showing how the component frequencies of the sound change in
intensity over time) using a mathematical technique called a <b>Fast Fourier Transform (FFT)</b>),
then broken into a series of overlapping chunks called <b>acoustic
frames</b>, each one typically lasting 1/25 to 1/50 of a second. These are digitally
processed in various ways and analyzed to find the components of
speech they contain. Assuming we've separated the utterance into words,
and identified the key features of each one, all we have to do is compare
what we have with a <b>phonetic dictionary</B> (a list of known words
and the sound fragments or features from which they're made)
and we can identify what's <i>probably</i> been said.
<I>Probably</I> is always the word in speech
recognition: no-one but the
speaker can ever know <i>exactly</i> what was said.)</p>
</div>

<div class="box">
<h2>Seeing speech</h2>

<p><img src="http://cdn4.explainthatstuff.com/sound-spectrogram-trace.jpg" class="irightno" height="170" width="600" ALT="A typical sound spectrogram, comparing three different sounds."></p>

<p>Speech recognition programs start by turning utterances into a <B>spectrogram</B>. It's a three-dimensional graph:
<UL>
<LI>Time is shown on the horizontal axis, flowing from left to right</LI>
<LI>Frequency is on the vertical axis, running from bottom to top</LI>
<LI>The color of the chart at each point shows how much energy there is in each frequency of the sound at a given moment.</LI>
</UL>
<P>
In this example, I've sung three distinct tones into a microphone, each one lasting about 5&ndash;10 seconds, with a bit of silence in between. The first one, shown by the small red area on the left, is the trace for a quiet, low-frequency sound. That's why the graph shows dark colors (reds and purples) concentrated in the bottom of the screen. The second tone, in the middle, is a similar tone to the first but quite a bit louder (which is why the colors appear a bit brighter). The third tone, on the right, has both a higher frequency and intensity. So the trace goes higher up the screen (higher frequencies) and the colors are brighter (more energy).</P>

<P>With a fair bit of practice, you could recognize what someone is saying just by looking at a diagram like this; indeed, it was <a href="https://archive.org/stream/ERIC_ED318062/ERIC_ED318062_djvu.txt" target="_blank">once believed</A> that deaf and hearing-impaired people might be trained to use spectrograms to help them decode words they couldn't hear.</p>
</div>

<div class="main2">
<p>In theory, since spoken languages are built from only a few dozen
phonemes (English uses about 46, while Spanish has only about 24),
you could recognize any possible spoken utterance just by learning to
pick out phones (or similar key features of spoken language such as
<b>formants</b>, which are prominent frequencies that can be used to
help identify vowels). Instead of having to recognize the sounds of
(maybe) 40,000 words, you'd only need to recognize the 46 basic
component sounds (or however many there are in your language), though
you'd still need a large phonetic dictionary listing the
phonemes that make up each word. This method of analyzing
spoken words by identifying phones or phonemes is often called the
<b>beads-on-a-string model</b>: a chunk of unknown speech (the
string) is recognized by breaking it into phones or bits of phones
(the beads); figure out the phones and you can figure out the words.</p>


<p>Most speech recognition programs get better as you use them
because they learn as they go along using feedback you give
them, either deliberately (by correcting mistakes) or by default
(if you don't correct any mistakes, you're effectively saying everything was
recognized perfectly&mdash;which is also feedback). If you've ever used a program like one of the
Dragon dictation systems, you'll be familiar with the way you have to
correct your errors straight away to ensure the program continues to
work with high accuracy. If you don't correct mistakes, the program
assumes it's recognized everything correctly, which means similar
mistakes are even more likely to happen next time. If you force the
system to go back and tell it which words it should have chosen, it
will associate those corrected words with the sounds it heard&mdash;and
do much better next time.</p>
<p><img src="http://cdn4.explainthatstuff.com/dictating-text-voice-recognition-software.png" class="irightno" height="250" width="400" ALT="An example of dictating text into a computer using Dragon Dictate voice recognition software."></p>

<p><span class="credit">Screenshot: With speech dictation programs like Dragon NaturallySpeaking, shown here,
it's important to go back and correct your mistakes if you want your words to be recognized accurately in future.</span>
</p>


<h3>3: Statistical analysis</h3>

<p>In practice, recognizing speech is much more complex than simply
identifying phones and comparing them to stored patterns, and for a
whole variety of reasons:</P>
<UL>
<LI>Speech is extremely variable: different people speak in different ways (even though we're all
saying the same words and, theoretically, they're all built from a
standard set of phonemes)</LI>
<LI>You don't always pronounce a certain word in exactly the same way; even if you did, the way you spoke a word
(or even part of a word) might vary depending on the sounds or words that came before or after.</LI>
<LI>As a speaker's vocabulary grows, the number of similar-sounding
words grows too: the digits zero through nine all sound different
when you speak them, but "zero" sounds like "hero," "one"
sounds like "none," "two" could mean "two," "to," or
"too"... and so on. So recognizing numbers is a tougher job for
voice dictation on a PC, with a general 50,000-word vocabulary, than
for an automated switchboard with a very specific, 10-word vocabulary
containing only the ten digits.</LI>
<LI>The more speakers a system has to
recognize, the more variability it's going to encounter and the
bigger the likelihood of making mistakes.</LI>
</UL>

<P>For something like an
off-the-shelf voice dictation program (one that listens to your voice
and types your words on the screen), simple pattern recognition is
clearly going to be a bit hit and miss. </span>The basic principle of
recognizing speech by identifying its component parts certainly holds
good, but we can do an even better job of it by taking into account
how language really works. In other words, we need to use what's
called a <b>language model</b>.</p>

<p>When people speak, they're not simply muttering a series of random
sounds. Every word you utter depends on the words that come before or
after. For example, unless you're a contrary kind of poet, the word
"example" is much more likely to follow words like "for,"
"an," "better," "good", "bad," and so on than words
like "octopus," "table," or even the word "example"
itself. Rules of grammar make it unlikely that a noun like "table"
will be spoken before another noun ("table example" isn't
something we say) while adjectives ("red," "good," "clear")
come before nouns and not after them ("good example" is far more
probable than "example good"). If a computer is trying to
figure out some spoken text and gets as far as hearing "here is a
******* example," it can be reasonably confident that ******* is an
adjective and not a noun. So it can use the rules of grammar to
exclude nouns like "table" and the probability of pairs like
"good example" and "bad example" to make an intelligent
guess. If it's already identified a "g" sound instead of a "b",
that's an added clue.</p>

<p>Virtually all modern speech recognition systems also use a bit of
complex statistical hocus-pocus to help figure out what's being said.
The probability of one phone following another, the probability of
bits of silence occurring in between phones, and the likelihood of
different words following other words are all factored in.
Ultimately, the system builds what's called a
<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">hidden Markov model</a>
(HMM) of each speech segment, which is the computer's best guess at
which beads are sitting on the string, based on all the things it's
managed to glean from the sound spectrum and all the bits and pieces
of phones and silence that it might reasonably contain. It's called a
Markov model (or Markov chain), for Russian mathematician
<a href="http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Markov.html" target="_blank">Andrey Markov</A>, because it's a sequence of different things (bits of phones, words, or whatever) that change from one
to the next with a certain probability. Confusingly, it's
referred to as a "hidden" Markov model even though it's worked out in
great detail and anything but hidden! "Hidden,"
in this case, simply means the contents of the model aren't observed directly but
figured out indirectly from the sound spectrum. From the computer's viewpoint, speech recognition is
always a probabilistic "best guess" and the right answer can never be known until the speaker
either accepts or corrects the words that have been recognized.
(Markov models can be processed with an extra bit of computer jiggery pokery called
the <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm" target="-blank">Viterbi algorithm</A>,
but that's beyond the scope of this article.)</p>

<h3>4: Artificial neural networks</h3>


<p><img src="http://cdn4.explainthatstuff.com/neural-network-structure.png" class="ileft" height="300" width="300" ALT="Artwork showing how a neural network is made up of input, hidden, and output units connected together."></p>

<p>HMMs have dominated speech recognition since the 1970s&mdash;for the
simple reason that they work so well. But they're by no means the
only technique we can use for recognizing speech. There's no reason
to believe that the brain itself uses anything like a hidden Markov
model. It's much more likely that we figure out what's being said
using dense layers of brain cells that excite and suppress one
another in intricate, interlinked ways according to the input signals
they receive from our cochleas (the parts of our inner ear that
recognize different sound frequencies).</p>

<p>Back in the 1980s, computer scientists developed "connectionist"
computer models that could mimic how the brain learns to recognize patterns,
which became known as <a href="introduction-to-neural-networks.html">artificial neural networks</a> (sometimes
called ANNs). A few speech recognition scientists explored using
neural networks, but the dominance and effectiveness of HMMs
relegated alternative approaches like this to the sidelines. More
recently, scientists have explored using ANNs and HMMs side by side
and found they give significantly higher accuracy over HMMs used
alone.</p>

<p><span class="credit">Artwork: Neural networks are hugely simplified, computerized versions of the brain&mdash;or a tiny part of it that have inputs (where you feed in information), outputs (where results appear), and hidden units (connecting the two). If you train them with enough examples, they learn by gradually adjusting the strength of the connections between the different layers of units. Once a neural network is fully trained, if you show it an unknown example, it will attempt to recognize what it is based on the examples it's seen before.</span></p>
</div>

<div class="box">
<h2>Speech recognition: a summary</h2>

<p><img src="http://cdn4.explainthatstuff.com/speech-recognition-stages.png" height="500" width="400" class="irightno" ALT="Summary of the key stages of speech recognition and some of the computational processes that are involved."></P>
<P>This artwork is a very quick summary of what we've explored so far. The blocks down the center represent
the path we follow from hearing an unknown bit of speech (at the top) to confidently declaring what we think has been said (at the bottom). It's a very general summary; not all speech recognition involves all these stages, in this exact order.</P>
<P>
The colored ovals down the sides represent some of the key computational processes that get us from unknown
utterance to recognized speech. Again, not all of these are used in every speech recognition system, they don't always happen in this order, and there are quite a few other things I've missed out (in an effort to keep my explanation reasonably short and simple). Generally speaking, though, the processes happen where I've positioned them. So the analog to digital and Fast Fourier Transform (FFT) stages happen quite early on, while the Hidden Markov Model (HMM) is built later. User feedback happens at the very end and corrects not just the recognized words but also things like the phonetic dictionary (how a certain speaker pronounces each word) or feature analysis program (each speaker will pronounce different things in different ways).</P>

<P>
Importantly, speech recognition software often works <I>recursively</I> (repeatedly moving back and forth) rather than in a single pass from the first word to the last. It's a bit like solving a crossword puzzle. The more clues you fill in, the more information you have, and the more constraints there are on the remaining clues. Equally, you may need to revisit some of your early answers, which turn out to be inconsistent with things you find out later. The closer you get to the end of a complete sentence, the easier it is to identify mistakes in the grammar or the syntax&mdash;and those could also force you to revisit your guesses at the earlier words in the sentence. In short, there's a lot of back-and-forth in speech recognition: the computational processes work in parallel, and "cooperate," to give the most accurate guess at the words in the spoken utterance. </P>

<p><span class="credit">Artwork: A summary of some of the key stages of speech recognition and the computational processes happening behind the scenes.</span></p>
</div>

<div class="main2">
<h2>What can we use speech recognition for?</h2>

<p>We've already touched on a few of the more common applications of
speech recognition, including automated telephone switchboards and
computerized voice dictation systems. But there are plenty more
examples where those came from.</p>

<p>Many of us (whether we know it or not) have <a href="cellphones.html">cellphones</a> with voice
recognition built into them. Back in the late 1990s, state-of-the-art
mobile phones offered <b>voice-activated dialing</b>,
where, in effect, you recorded a sound snippet for each entry
in your phonebook, such as the spoken word "Home," or whatever
that the phone could then recognize when you spoke it in future. A
few years later, systems like SpinVox became popular helping mobile
phone users make sense of voice messages by converting them
automatically into text (although a 
<a href="http://news.bbc.co.uk/1/hi/technology/8174721.stm" target="_blank">sneaky BBC investigation eventually claimed</a> that some of its state-of-the-art speech automated
speech recognition was actually being done by humans in developing
countries!).</p>

<p>Today's smartphones make voice recognition even more of a feature.
Apple's <a href="http://www.apple.com/ios/siri/" target="_blank">Siri</A>,
Google's <a href="https://www.google.co.uk/landing/now/" target="_blank">Now</A>, and Microsoft's
<a href="http://www.windowsphone.com/en-us/how-to/wp8/cortana/meet-cortana" target="_blank">Cortana</A> are  smartphone "personal assistant apps" who'll listen to what you say, figure out what you mean, then
attempt to do what you ask, whether it's looking up a phone number or
booking a table at a local restaurant. They work by linking speech
recognition to complex <a href="http://en.wikipedia.org/wiki/Natural_language_processing" target="_blank">natural language processing (NLP)</A> systems, so they can figure out not just what you <i>say</i>, but what you
actually <i>mean</i>, and what you really want to <i>happen</i> as a
consequence. Pressed for time and hurtling down the street, mobile
users theoretically find this kind of system a boon&mdash;at least if you
believe the hype in the TV advertisements that Google and Microsoft
have been running to promote their systems. But it's not just desktop
users who benefit from being able to ask for what they want: Google
quietly incorporated speech recognition into its search engine some
time ago, so you can Google without going anywhere near your
keyboard, if you really want to.</p>


<h2>Will speech recognition ever take off?</h2>

<p>I'm a huge fan of speech recognition. After suffering with
repetitive strain injury on and off for some time, I've been using
computer dictation to write quite a lot of my stuff for about 15
years, and it's been amazing to see the improvements in off-the-shelf
voice dictation over that time. The early Dragon Dictate system I
used on a Windows 95 laptop was fairly reliable, but I had to speak
relatively slowly, pausing slightly between each word or word group,
giving a horribly staccato style that tended to interrupt my train of
thought. This slow, tedious one-word-at-a-time approach ("can &ndash;
you &ndash; tell &ndash; what &ndash; I &ndash; am &ndash; saying &ndash; to &ndash; you") went
by the name <b>discrete speech recognition</b>. A few years later,
things had improved so much that virtually all the off-the-shelf
programs like Dragon were offering <b>continuous speech recognition</b>,
which meant I could speak at normal speed, in a normal way, and still
be assured of very accurate word recognition. When you can speak
normally to your computer, at a normal talking pace, voice dictation
programs offer another advantage: they give clumsy, self-conscious
writers a much more attractive, conversational style: "write like
you speak" (always a good tip for writers) is easy to put into
practice when you speak <i>all</i> your words as you write them!</p>

<p>Despite the technological advances, I still generally prefer to
write with a <a href="computerkeyboards.html">keyboard</A> and
<a href="computermouse.html">mouse</A>. Ironically, I'm writing this article
that way now. Why? Partly because it's what I'm used to. I often
write highly technical stuff with a complex vocabulary that I know
will defeat the best efforts of all those hidden Markov models and
neural networks battling away inside my PC. It's easier to type
"hidden Markov model" than to mutter those words somewhat
hesitantly, watch "hiccup half a puddle" pop up on screen and
then have to make corrections.</p>

<p><img src="http://cdn4.explainthatstuff.com/speech-recognition-training.png" class="ileftno" height="319" width="523" ALT="Training a speech recognition program to recognize the words 'hidden Markov model.'"></p>

<p><span class="credit">Screenshot: You an always add more words to a speech recognition program. Here, I've decided to train Microsoft Vista's built-in speech recognition engine to spot the words 'hidden Markov model.'</span>
</p>


<h3>Mobile revolution?</h3>

<p>You might think mobile devices&mdash;with their slippery <a href="touchscreens.html">touchscreens</A>&mdash;would benefit enormously from speech recognition:
no-one really wants to type an essay with two thumbs on a pop-up QWERTY keyboard.
Ironically, mobile devices are heavily used by younger,
tech-savvy kids who prefer typing and pawing at screens to speaking
out loud. Why? All sorts of reasons, from sheer familiarity (it's
quick to type once you're used to it&mdash;and faster
than fixing a computer's goofed-up guesses) to privacy and consideration for others (many of us
use our mobile phones in public places and we don't want our thoughts wide open to scrutiny
or howls of derision), and the sheer difficulty of speaking clearly
and being clearly understood in noisy environments. What you're
doing with your computer also makes a difference. If you've ever used
voice recognition on a PC, you'll know that <i>writing</i> something
like an essay (dictating hundreds or thousands of words of ordinary
text) is a whole lot easier than <i>editing</i> it afterwards (where
you laboriously try to select words or sentences and move them up or
down so many lines with awkward cut and paste commands). And trying
to open and close windows, start programs, or navigate around a
computer screen by voice alone is clumsy, tedious, error-prone, and
slow. It's far easier just to click your mouse or swipe your finger.</p>

<p>Developers of speech recognition systems insist everything's about
to change, largely thanks to natural language processing and smart
search engines that can understand spoken queries.
But people have been saying that for decades now: the brave new
world is always just around the corner. According to speech pioneer James Baker,
better speech recognition "would greatly increase the speed and ease with which humans could
communicate with computers, and greatly speed and ease the ability
with which humans could record and organize their own words and
thoughts"&mdash;but he wrote (or perhaps voice dictated?) those words 25 years ago!
Just because Google can now understand speech, it doesn't follow that we automatically want
to speak our queries rather than type them&mdash;especially when you consider some of
the wacky things people look for online. Humans didn't invent written language
because others struggled to hear and understand what they were
saying. Writing and speaking serve different purposes. Writing is a
way to set out longer, more clearly expressed and elaborated thoughts
without having to worry about the limitations of your short-term
memory; speaking is much more off-the-cuff. Writing is introverted,
intimate, and inherently private; it's carefully and thoughtfully composed.
Speaking is an altogether different way of expressing your thoughts&mdash;and people don't always want to
speak their minds. While technology may be ever advancing, it's far
from certain that voice recognition will ever take off in quite the
way that its developers would like. I'm typing these words, after
all, not speaking them.</p>



<div class="sharebot"><ul class="nobull">
<li class="shr"><iframe src="http://www.facebook.com/plugins/like.php?app_id=214994648542122&amp;href=http%3A%2F%2Fwww.explainthatstuff.com%2Fvoicerecognition.html&amp;send=false&amp;layout=button_count&amp;width=90&amp;show_faces=false&amp;action=like&amp;colorscheme=light&amp;font&amp;height=21" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:90px; height:21px;" allowTransparency="true"></iframe></li>
<!-- plusone and tweet -->
<!-- sharebuttons -->
<li class="shr"><g:plusone size="medium"></g:plusone></li>
<li class="shr"><a href="http://twitter.com/share" class="twitter-share-button" data-count="horizontal">Tweet</a></li> 
</ul></div>

<h2>Find out more</h2>


<h3>On this website</h3>
<UL>
<li><a href="howcomputerswork.html">Computers</a></li>
<li><a href="computerkeyboards.html">Keyboards</a></li>
<li><a href="microphones.html">Microphones</a></li>
<li><a href="introduction-to-neural-networks.html">Neural networks</a></li>
<li><a href="how-speech-synthesis-works.html">Speech synthesis</a></li>
</UL>

<h3>Books</h3>
<UL>
<li><a href="https://books.google.com/books?id=fZmj5UNK8AQC" target="_blank">Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</a> by Daniel Jurafsky, James Martin. Prentice Hall, 2009. An up-to-date, interdisciplinary review of speech recognition technology.</li>
<li><a href="http://books.google.com/books?id=uGBTAAAAMAAJ" target="_blank">Voice Recognition</a> by Richard L. Klevans and Robert D. Rodman. Artech House, 1997. A short introduction to the science of voice recognition.</li>
<li><a href="http://books.google.com/books?id=1C9dzcJTWowC" target="_blank">Statistical Methods for Speech Recognition</A> by Frederick Jelinek. MIT Press, 1997. A detailed guide to Hidden Markov Models and the other statistical techniques that computers use to figure out human speech.</li>
<li><a href="http://books.google.com/books?id=XEVqQgAACAAJ" target="_blank">Fundamentals of Speech Recognition</A> by Lawrence R. Rabiner and Biing-Hwang Juang. PTR Prentice Hall, 1993. A little dated now, but still a good introduction to the basic concepts.</li>
<li><a href="https://books.google.com/books?id=eJac7g7YfZIC" target="_blank">Speech Recognition: Invited Papers Presented at the 1974 IEEE Symposium</a> by D. R. Reddy (ed). Academic Press, 1975. A classic collection of pioneering papers from the golden age of the 1970s.</li>
</UL>

<h3>Articles</h3>
<h4>News</h4>
<UL>
<li><a href="http://research.microsoft.com/en-us/news/features/speechrecognition-082911.aspx" target="_blank">The Holy Grail of Speech Recognition</a> by Janie Chang: Microsoft Research, 29 August 2011. How neural networks are making a comeback in speech recognition research.</li>
<li><a href="http://www.novaspivack.com/technology/how-hisiri-works-interview-with-tom-gruber-cto-of-siri" target="_blank">How Siri Works: Interview with Tom Gruber</a> by Nova Spivack, Minding the Planet, 26 January 2010. Gruber explains some of the technical tricks that allow Siri to understand natural language.</li>
<li><a href="http://news.bbc.co.uk/1/hi/programmes/click_online/8609723.stm" target="_blank">Why machines do not understand human speech</a>: BBC News, 9 April 2010. Why is it so hard for computers to make sense of people?</li>
<li><a href="http://news.bbc.co.uk/1/hi/programmes/click_online/8052004.stm" target="_blank">A sound start for speech tech</a>: by LJ Rich. BBC News, 15 May 2009. Cambridge University's Dr Tony Robinson talks us through the science of voice recognition.</li>
</UL>

<h4>Popular science</h4>
<UL>
<li><a href="http://www.scientificamerican.com/article/speech-getting-computers-understand-overlapping/" target="_blank">Audio Alchemy: Getting Computers to Understand Overlapping Speech</a> by John R. Hershey et al. Scientific American, April 12, 2011. How can computers make sense of two people talking at once?</li>
</UL>

<h4>Technical</h4>
<UL>
<li><a href="http://cacm.acm.org/magazines/2014/1/170863-a-historical-perspective-of-speech-recognition/abstract" target="_blank">A Historical Perspective of Speech Recognition</a> by Xuedong Huang, James Baker, Raj Reddy. Communications of the ACM, January 2014 (Vol. 57 No. 1), Pages 94&ndash;103.</li>
<li><a href="http://research.microsoft.com/pubs/144412/dbn4lvcsr-transaslp.pdf" target="_blank">Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition</a> by George Dahl et al. IEEE Transactions on Audio, Speech, and Language Processing, Vol. 20 No. 1, January 2012. A review of Microsoft's recent research into using neural networks with HMMs. [PDF format]</li>
<li><a href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=18626" target="_blank">A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition</a> by Lawrence R. Rabiner. Proceedings of the IEEE, Vol 77 No 2, February 1989. A classic introduction to Markov models, though non-mathematicians will find it tough going.</li>
</UL>

<h3>Patents</h3>

<P>Patents set out the technical gritty of how inventions work. There are dozens covering different kinds of speech recognition; here are a few representative examples:</P>
<UL>
<li><a href="http://www.google.com/patents/US4783803" target="_blank">US Patent: 4,783,803: Speech recognition apparatus and method</a> by James K. Baker, Dragon Systems, 8 November 1988. One of Baker's first Dragon patents. Another Baker patent filed the following year follows on from this. See <a href="https://www.google.com/patents/US4866778" target="_blank">US Patent: 4,866,778: Interactive speech recognition apparatus</a> by James K. Baker, Dragon Systems, 12 September 1989.</li>
<li><a href="http://www.google.com/patents/US4587670" target="_blank">US Patent 4,783,804: Hidden Markov model speech recognition arrangement</a> by Stephen E. Levinson, Lawrence R. Rabiner, and Man M. Sondi, AT&amp;T Bell Laboratories, 6 May 1986. Sets out one approach to probabilistic speech recognition using Markov models.</li>
<li><a href="http://www.google.com/patents/US4363102" target="_blank">US Patent: 4,363,102: Speaker identification system using word recognition templates</a> by John E. Holmgren, Bell Labs, 7 December 1982. A method of recognizing a particular person's voice using analysis of key features.</li>
<li><a href="https://www.google.co.uk/patents/US2938079" target="_blank">US Patent 2,938,079: Spectrum segmentation system for the automatic extraction of formant frequencies from human speech</a> by James L. Flanagan, US Air Force, 24 May 1960. An early speech recognition system based on formant (peak frequency) analysis.</li>

</UL>

<h3>Videos</h3>
<UL>
<li><a href="http://vimeo.com/81229833" target="_blank">A Historical Perspective of Speech Recognition</a> by Raj Reddy (an AI researcher at Carnegie Mellon), James Baker (founder of Dragon), and Xuedong Huang (of Microsoft). Speech recognition pioneers look back on the advances they helped to inspire in this four-minute discussion.</li>
</UL>



</div>


<div class="main2">
<h2>If you liked this article...</h2>
<P>You might like my new book, <a href="mybooks.php">Atoms Under the Floorboards: The Surprising Science Hidden in Your Home</a>, published worldwide by Bloomsbury.</P>
</div>

<!-- Desktop bottom ad -->
<div class="bottomsquare"><span class="slinks2">Sponsored links</span>
<P>
<script async src="http://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:inline-block;width:336px;height:280px"
     data-ad-client="ca-pub-1030585152417294"
     data-ad-slot="9721739364"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
</P>
</div>


<div class="cc"><P><span class="highlight">Please do NOT copy our articles onto blogs and other websites</span></P>
<p>Text copyright &copy; Chris Woodford 2006, 2015. All rights reserved. <a href="copyright-and-legal-notices.html">Full copyright notice and terms of use</A>.</p>



</div>


<div class="sharesites"><h2>Follow us</h2>
<ul class="nobull">
<LI class="inline2"><a href="followus.html" title="Follow us on Facebook"><IMG class="sprite shr4" ALT="Follow us on Facebook" src="http://cdn4.explainthatstuff.com/dot.gif"></A></li>
<LI class="inline2"><a rel="publisher" href="https://plus.google.com/115402323314399894520" target="_blank" title="Follow us on Google+"><IMG class="sprite shr9" ALT="Google+"  src="http://cdn4.explainthatstuff.com/dot.gif"></A></li>
<LI class="inline2"><a href="rss.xml" target="_blank" title="Subscribe to our RSS feed"><IMG class="sprite shr10" ALT="RSS"  src="http://cdn4.explainthatstuff.com/dot.gif"></A></li>
<LI class="inline2"><a href="http://www.flickr.com/photos/explainthatstuff" target="_blank" title="Find our photos on Flickr"><IMG class="sprite shr11" ALT="Flickr"  src="http://cdn4.explainthatstuff.com/dot.gif"></A></li>
<LI class="inline2"><a href="http://www.pinterest.com/explainthatstuf/" title="Follow us on Pinterest" target="_blank"><IMG class="sprite shr12" ALT="Follow us on Pinterest" src="http://cdn4.explainthatstuff.com/dot.gif"></A></li>
</ul>
</div>

<div class="sharesites"><h2>Rate this page</h2>
<p>Please <a href="feedback.php">rate or give feedback on this page</A> and I will make a donation to WaterAid.</P></div>



<div class="sharesites">
<h2>Share this page</h2> 
<p>Press CTRL + D to bookmark this page for later or tell your friends about it with:</p>

<ul class="nobull">
<LI class="inline2"><a title="Share this on Facebook" href="http://www.facebook.com/sharer.php?u=http://www.explainthatstuff.com/voicerecognition.html"><IMG class="sprite shr4" ALT="Facebook" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI> 
<LI class="inline2">
<a title="Post this page to Delicious" href="http://del.icio.us/post?url=http://www.explainthatstuff.com/voicerecognition.html&amp;title=How%20voice%20recognition%20works"><IMG class="sprite shr1" ALT="Delicious" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Share this on Digg" href="http://digg.com/submit?url=http://www.explainthatstuff.com/voicerecognition.html&amp;title=How%20voice%20recognition%20works"><IMG class="sprite shr2" ALT="Digg" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Share this on Reddit" href="http://reddit.com/submit?url=http://www.explainthatstuff.com/voicerecognition.html&amp;title=How%20voice%20recognition%20works"><IMG class="sprite shr3" ALT="Reddit" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Share this on StumbleUpon" href="http://www.stumbleupon.com/submit?url=http://www.explainthatstuff.com/voicerecognition.html&amp;title=How%20voice%20recognition%20works"><IMG class="sprite shr5" ALT="StumbleUpon" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI> 
<LI class="inline2"><a title="Add this to your Google Bookmarks"
 href="http://www.google.com/bookmarks/mark?op=edit&amp;bkmk=http://www.explainthatstuff.com/voicerecognition.html&amp;title=How%20voice%20recognition%20works"><IMG class="sprite shr6" ALT="Google Bookmarks" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a href="http://www.twitter.com/home?status=How+voice+recognition+software+works+http://www.explainthatstuff.com/voicerecognition.html" title="Tweet this"><IMG class="sprite shr7" ALT="Twitter" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a title="Email this page to a friend"
 href="mailto:?subject=How%20voice%20recognition%20works&amp;body=Take%20a%20look%20at%20this%20page:%20http://www.explainthatstuff.com/voicerecognition.html"><IMG class="sprite shr8" ALT="Email" src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI>
<LI class="inline2"><a href="https://plus.google.com/101536672158385330677?rel=author" title="Connect on Google+"><IMG class="sprite shr9" ALT="Google+"  src="http://cdn4.explainthatstuff.com/dot.gif"></a></LI>
</UL>


<h2>Cite this page</h2>

<!---Citation-->
<div class="citation">
<P>
Woodford, Chris. (2006/2015) Voice recognition software. Retrieved from http://www.explainthatstuff.com/voicerecognition.html. [Accessed (Insert date here)]
</P>
</div>



</div>


<!-- advinfo2 -->

<!-- bottom search position -->



<div class="alsoon"><a name="navigation"></a>
<IMG class="footlogo" ALT="Explain that Stuff" src="http://cdn4.explainthatstuff.com/dot.gif">
<h2>More to explore on our website...</h2>
<ul class="nobull">
<LI class="inline"><a href="articles_communications.html">Communications</A></LI>
<LI class="inline"><a href="articles_computers.html">Computers</A></LI>
<LI class="inline"><a href="articles_electricity.html">Electricity &amp; electronics</A></LI>
<LI class="inline"><a href="articles_energy.html">Energy</A></LI>
<LI class="inline"><a href="articles_engineering.html">Engineering</A></LI>
<LI class="inline2"><a href="articles_environment.html">Environment</A></LI><BR>
<LI class="inline"><a href="articles_gadgets.html">Gadgets</A></LI>
<LI class="inline"><a href="articles_homelife.html">Home life</A></LI>
<LI class="inline"><a href="articles_materials.html">Materials</A></LI>
<LI class="inline"><a href="articles_science.html">Science</A></LI>
<LI class="inline"><a href="articles_tools.html">Tools &amp; instruments</A></LI>
<LI class="inline2"><a href="articles_transportation.html">Transportation</A></LI>
</UL>
</div>

<div class="footer">
<ul class="nobull"><li class="inline"><a href="/">Home</a></li>
<li class="inline"><a href="azindex.html">A-Z index</a></li>
<li class="inline"><a href="mybooks.php">Get the book</a></li>
<li class="inline"><a href="followus.html">Follow us</a></li>
<li class="inline"><a href="random.php">Random article</a></li>
<li class="inline"><a href="timeline.html">Timeline</a></li>
<li class="inline"><a href="teaching-guide.html">Teaching guide</a></li>
<li class="inline"><a href="aboutus.html">About us</a></li>
<li class="inline2"><a href="privacy.html">Privacy policy</a></li></ul>
</div>

<div class="totop">&uarr; <a href="#pagetop">Back to top</a></div>
</div>







<script type="text/javascript">
(function(d, t) {
var g = d.createElement(t),
        s = d.getElementsByTagName(t)[0];
g.async = true;
g.src = 'https://apis.google.com/js/plusone.js';
s.parentNode.insertBefore(g, s);
})(document, 'script');
</script>

<script type="text/javascript">
  (function(){
    var twitterWidgets = document.createElement('script');
    twitterWidgets.type = 'text/javascript';
    twitterWidgets.async = true;
    twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
    document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
  })();
</script>

<script type="text/javascript" src="http://www.google.com/coop/cse/brand?form=cse-search-box&amp;lang="></script>


</body>
</html>
